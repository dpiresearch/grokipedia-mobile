\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Heslow, Launay, Malartic, Noune, Pannier, and Penedo]{falcon}
E.~Almazrouei, H.~Alobeidli, A.~Alshamsi, A.~Cappelli, R.~Cojocaru, M.~Debbah, E.~Goffinet, D.~Heslow, J.~Launay, Q.~Malartic, B.~Noune, B.~Pannier, and G.~Penedo.
\newblock {Falcon-40B}: an open large language model with state-of-the-art performance, 2023.

\bibitem[Artetxe et~al.(2022)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer, Lin, Du, Iyer, Pasunuru, Anantharaman, Li, Chen, Akin, Baines, Martin, Zhou, Koura, O'Horo, Wang, Zettlemoyer, Diab, Kozareva, and Stoyanov]{moe_ft_bad}
M.~Artetxe, S.~Bhosale, N.~Goyal, T.~Mihaylov, M.~Ott, S.~Shleifer, X.~V. Lin, J.~Du, S.~Iyer, R.~Pasunuru, G.~Anantharaman, X.~Li, S.~Chen, H.~Akin, M.~Baines, L.~Martin, X.~Zhou, P.~S. Koura, B.~O'Horo, J.~Wang, L.~Zettlemoyer, M.~T. Diab, Z.~Kozareva, and V.~Stoyanov.
\newblock Efficient large scale language modeling with mixtures of experts.
\newblock In Y.~Goldberg, Z.~Kozareva, and Y.~Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022}, pages 11699--11732. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.EMNLP-MAIN.804}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.emnlp-main.804}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{mbpp}
J.~Austin, A.~Odena, M.~Nye, M.~Bosma, H.~Michalewski, D.~Dohan, E.~Jiang, C.~Cai, M.~Terry, Q.~Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{pythia}
S.~Biderman, H.~Schoelkopf, Q.~G. Anthony, H.~Bradley, K.~O'Brien, E.~Hallahan, M.~A. Khan, S.~Purohit, U.~S. Prashanth, E.~Raff, A.~Skowron, L.~Sutawika, and O.~van~der Wal.
\newblock Pythia: {A} suite for analyzing large language models across training and scaling.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and J.~Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 2397--2430. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/biderman23a.html}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{piqa}
Y.~Bisk, R.~Zellers, R.~L. Bras, J.~Gao, and Y.~Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pages 7432--7439. {AAAI} Press, 2020.
\newblock \doi{10.1609/aaai.v34i05.6239}.
\newblock URL \url{https://doi.org/10.1609/aaai.v34i05.6239}.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt_neo}
S.~Black, L.~Gao, P.~Wang, C.~Leahy, and S.~Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}, Mar. 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5297715}.
\newblock {If you use this misc, please cite it using these metadata.}

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert{-}Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{codex}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. de~Oliveira~Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger, M.~Petrov, H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder, M.~Pavlov, A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P. Such, D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert{-}Voss, W.~H. Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji, S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra, E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer, P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and W.~Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and O.~Tafjord.
\newblock Think you have solved question answering? try arc, the {AI2} reasoning challenge.
\newblock \emph{CoRR}, abs/1803.05457, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.05457}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dai et~al.(2022{\natexlab{a}})Dai, Dong, Hao, Sui, Chang, and Wei]{kn}
D.~Dai, L.~Dong, Y.~Hao, Z.~Sui, B.~Chang, and F.~Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock In S.~Muresan, P.~Nakov, and A.~Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pages 8493--8502. Association for Computational Linguistics, 2022{\natexlab{a}}.
\newblock \doi{10.18653/V1/2022.ACL-LONG.581}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.acl-long.581}.

\bibitem[Dai et~al.(2022{\natexlab{b}})Dai, Dong, Ma, Zheng, Sui, Chang, and Wei]{stablemoe}
D.~Dai, L.~Dong, S.~Ma, B.~Zheng, Z.~Sui, B.~Chang, and F.~Wei.
\newblock Stablemoe: Stable routing strategy for mixture of experts.
\newblock In S.~Muresan, P.~Nakov, and A.~Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pages 7085--7095. Association for Computational Linguistics, 2022{\natexlab{b}}.
\newblock \doi{10.18653/V1/2022.ACL-LONG.489}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.acl-long.489}.

\bibitem[DeepSeek-AI(2024)]{deepseek_llm}
DeepSeek-AI.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock \emph{arXiv preprint arXiv:2401.02954}, 2024.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{glam}
N.~Du, Y.~Huang, A.~M. Dai, S.~Tong, D.~Lepikhin, Y.~Xu, M.~Krikun, Y.~Zhou, A.~W. Yu, O.~Firat, B.~Zoph, L.~Fedus, M.~P. Bosma, Z.~Zhou, T.~Wang, Y.~E. Wang, K.~Webster, M.~Pellat, K.~Robinson, K.~S. Meier{-}Hellstern, T.~Duke, L.~Dixon, K.~Zhang, Q.~V. Le, Y.~Wu, Z.~Chen, and C.~Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 5547--5569. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/du22c.html}.

\bibitem[Dua et~al.(2019)Dua, Wang, Dasigi, Stanovsky, Singh, and Gardner]{drop}
D.~Dua, Y.~Wang, P.~Dasigi, G.~Stanovsky, S.~Singh, and M.~Gardner.
\newblock {DROP:} {A} reading comprehension benchmark requiring discrete reasoning over paragraphs.
\newblock In J.~Burstein, C.~Doran, and T.~Solorio, editors, \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}, pages 2368--2378. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/N19-1246}.
\newblock URL \url{https://doi.org/10.18653/v1/n19-1246}.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{switch}
W.~Fedus, B.~Zoph, and N.~Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{CoRR}, abs/2101.03961, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.03961}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{pile}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, et~al.
\newblock The {Pile}: An {800GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Geng and Liu(2023)]{openllama}
X.~Geng and H.~Liu.
\newblock Openllama: An open reproduction of llama, May 2023.
\newblock URL \url{https://github.com/openlm-research/open_llama}.

\bibitem[Harlap et~al.(2018)Harlap, Narayanan, Phanishayee, Seshadri, Devanur, Ganger, and Gibbons]{pipedream}
A.~Harlap, D.~Narayanan, A.~Phanishayee, V.~Seshadri, N.~R. Devanur, G.~R. Ganger, and P.~B. Gibbons.
\newblock Pipedream: Fast and efficient pipeline parallel {DNN} training.
\newblock \emph{CoRR}, abs/1806.03377, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.03377}.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{math}
D.~Hendrycks, C.~Burns, S.~Kadavath, A.~Arora, S.~Basart, E.~Tang, D.~Song, and J.~Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset, 2021.

\bibitem[High-Flyer(2023)]{haillm}
High-Flyer.
\newblock Hai-llm: An efficient and lightweight tool for training large models, 2023.
\newblock URL \url{https://www.high-flyer.cn/en/blog/hai-llm}.

\bibitem[Hochreiter and Schmidhuber(1997)]{lstm}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computing}, 9\penalty0 (8):\penalty0 1735--1780, 1997.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.8.1735}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{scaling_law}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford, D.~de~Las~Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, T.~Hennigan, E.~Noland, K.~Millican, G.~van~den Driessche, B.~Damoc, A.~Guy, S.~Osindero, K.~Simonyan, E.~Elsen, J.~W. Rae, O.~Vinyals, and L.~Sifre.
\newblock Training compute-optimal large language models.
\newblock \emph{CoRR}, abs/2203.15556, 2022.
\newblock \doi{10.48550/arXiv.2203.15556}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2203.15556}.

\bibitem[Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, et~al.]{ceval}
Y.~Huang, Y.~Bai, Z.~Zhu, J.~Zhang, J.~Zhang, T.~Su, J.~Liu, C.~Lv, Y.~Zhang, J.~Lei, et~al.
\newblock {C-Eval}: A multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock \emph{arXiv preprint arXiv:2305.08322}, 2023.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{ori_moe1}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, and G.~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computing}, 3\penalty0 (1):\penalty0 79--87, 1991.
\newblock URL \url{https://doi.org/10.1162/neco.1991.3.1.79}.

\bibitem[Jordan and Jacobs(1994)]{ori_moe2}
M.~I. Jordan and R.~A. Jacobs.
\newblock Hierarchical mixtures of experts and the {EM} algorithm.
\newblock \emph{Neural Computing}, 6\penalty0 (2):\penalty0 181--214, 1994.
\newblock URL \url{https://doi.org/10.1162/neco.1994.6.2.181}.

\bibitem[{Joshi} et~al.(2017){Joshi}, {Choi}, {Weld}, and {Zettlemoyer}]{triviaqa}
M.~{Joshi}, E.~{Choi}, D.~{Weld}, and L.~{Zettlemoyer}.
\newblock {triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}.
\newblock \emph{arXiv e-prints}, art. arXiv:1705.03551, 2017.

\bibitem[Korthikanti et~al.(2023)Korthikanti, Casper, Lym, McAfee, Andersch, Shoeybi, and Catanzaro]{megatron3}
V.~A. Korthikanti, J.~Casper, S.~Lym, L.~McAfee, M.~Andersch, M.~Shoeybi, and B.~Catanzaro.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5, 2023.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Kelcey, Devlin, Lee, Toutanova, Jones, Chang, Dai, Uszkoreit, Le, and Petrov]{nq}
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~Parikh, C.~Alberti, D.~Epstein, I.~Polosukhin, M.~Kelcey, J.~Devlin, K.~Lee, K.~N. Toutanova, L.~Jones, M.-W. Chang, A.~Dai, J.~Uszkoreit, Q.~Le, and S.~Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association of Computational Linguistics}, 2019.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{race}
G.~Lai, Q.~Xie, H.~Liu, Y.~Yang, and E.~H. Hovy.
\newblock {RACE:} large-scale reading comprehension dataset from examinations.
\newblock In M.~Palmer, R.~Hwa, and S.~Riedel, editors, \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September 9-11, 2017}, pages 785--794. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/D17-1082}.
\newblock URL \url{https://doi.org/10.18653/v1/d17-1082}.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{gshard}
D.~Lepikhin, H.~Lee, Y.~Xu, D.~Chen, O.~Firat, Y.~Huang, M.~Krikun, N.~Shazeer, and Z.~Chen.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=qrwe7XHTmYb}.

\bibitem[Li et~al.(2023)Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin]{cmmlu}
H.~Li, Y.~Zhang, F.~Koto, Y.~Yang, H.~Zhao, Y.~Gong, N.~Duan, and T.~Baldwin.
\newblock {CMMLU}: Measuring massive multitask language understanding in {Chinese}.
\newblock \emph{arXiv preprint arXiv:2306.09212}, 2023.

\bibitem[Lin et~al.(2021)Lin, Men, Yang, Zhou, Ding, Zhang, Wang, Wang, Jiang, Jia, Zhang, Zhang, Zou, Li, Deng, Liu, Xue, Zhou, Ma, Yu, Li, Lin, Zhou, Tang, and Yang]{m6}
J.~Lin, R.~Men, A.~Yang, C.~Zhou, M.~Ding, Y.~Zhang, P.~Wang, A.~Wang, L.~Jiang, X.~Jia, J.~Zhang, J.~Zhang, X.~Zou, Z.~Li, X.~Deng, J.~Liu, J.~Xue, H.~Zhou, J.~Ma, J.~Yu, Y.~Li, W.~Lin, J.~Zhou, J.~Tang, and H.~Yang.
\newblock {M6:} {A} chinese multimodal pretrainer.
\newblock \emph{CoRR}, abs/2103.00823, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.00823}.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{truthfulqa}
S.~Lin, J.~Hilton, and O.~Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock In S.~Muresan, P.~Nakov, and A.~Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pages 3214--3252. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.ACL-LONG.229}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.acl-long.229}.

\bibitem[Loshchilov and Hutter(2019)]{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, et~al.]{megatron2}
D.~Narayanan, M.~Shoeybi, J.~Casper, P.~LeGresley, M.~Patwary, V.~Korthikanti, D.~Vainbrand, P.~Kashinkunti, J.~Bernauer, B.~Catanzaro, et~al.
\newblock Efficient large-scale language model training on gpu clusters using megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--15, 2021.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/arXiv.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He]{zero}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He.
\newblock Zero: memory optimizations toward training trillion parameter models.
\newblock In C.~Cuicchi, I.~Qualters, and W.~T. Kramer, editors, \emph{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020}, page~20. {IEEE/ACM}, 2020.
\newblock \doi{10.1109/SC41405.2020.00024}.
\newblock URL \url{https://doi.org/10.1109/SC41405.2020.00024}.

\bibitem[Rajbhandari et~al.(2022)Rajbhandari, Li, Yao, Zhang, Aminabadi, Awan, Rasley, and He]{deepspeed_moe}
S.~Rajbhandari, C.~Li, Z.~Yao, M.~Zhang, R.~Y. Aminabadi, A.~A. Awan, J.~Rasley, and Y.~He.
\newblock Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation {AI} scale.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesv{\'{a}}ri, G.~Niu, and S.~Sabato, editors, \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 18332--18346. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/rajbhandari22a.html}.

\bibitem[Ren et~al.(2023)Ren, Zhou, Meng, Huang, Wang, Wang, Li, Zhang, Podolskiy, Arshinov, Bout, Piontkovskaya, Wei, Jiang, Su, Liu, and Yao]{pangu_sigma}
X.~Ren, P.~Zhou, X.~Meng, X.~Huang, Y.~Wang, W.~Wang, P.~Li, X.~Zhang, A.~Podolskiy, G.~Arshinov, A.~Bout, I.~Piontkovskaya, J.~Wei, X.~Jiang, T.~Su, Q.~Liu, and J.~Yao.
\newblock Pangu-{\(\Sigma\)}: Towards trillion parameter language model with sparse heterogeneous computing.
\newblock \emph{CoRR}, abs/2303.10845, 2023.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.10845}.

\bibitem[Roller et~al.(2021)Roller, Sukhbaatar, Szlam, and Weston]{hash}
S.~Roller, S.~Sukhbaatar, A.~Szlam, and J.~Weston.
\newblock Hash layers for large sparse models.
\newblock \emph{CoRR}, abs/2106.04426, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.04426}.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{winogrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale, 2019.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ilic, Hesslow, Castagn{\'{e}}, Luccioni, Yvon, Gall{\'{e}}, Tow, Rush, Biderman, Webson, Ammanamanchi, Wang, Sagot, Muennighoff, del Moral, Ruwase, Bawden, Bekman, McMillan{-}Major, Beltagy, Nguyen, Saulnier, Tan, Suarez, Sanh, Lauren{\c{c}}on, Jernite, Launay, Mitchell, Raffel, Gokaslan, Simhi, Soroa, Aji, Alfassy, Rogers, Nitzav, Xu, Mou, Emezue, Klamm, Leong, van Strien, Adelani, and et~al.]{bloom}
T.~L. Scao, A.~Fan, C.~Akiki, E.~Pavlick, S.~Ilic, D.~Hesslow, R.~Castagn{\'{e}}, A.~S. Luccioni, F.~Yvon, M.~Gall{\'{e}}, J.~Tow, A.~M. Rush, S.~Biderman, A.~Webson, P.~S. Ammanamanchi, T.~Wang, B.~Sagot, N.~Muennighoff, A.~V. del Moral, O.~Ruwase, R.~Bawden, S.~Bekman, A.~McMillan{-}Major, I.~Beltagy, H.~Nguyen, L.~Saulnier, S.~Tan, P.~O. Suarez, V.~Sanh, H.~Lauren{\c{c}}on, Y.~Jernite, J.~Launay, M.~Mitchell, C.~Raffel, A.~Gokaslan, A.~Simhi, A.~Soroa, A.~F. Aji, A.~Alfassy, A.~Rogers, A.~K. Nitzav, C.~Xu, C.~Mou, C.~Emezue, C.~Klamm, C.~Leong, D.~van Strien, D.~I. Adelani, and et~al.
\newblock {BLOOM:} {A} 176b-parameter open-access multilingual language model.
\newblock \emph{CoRR}, abs/2211.05100, 2022.
\newblock \doi{10.48550/ARXIV.2211.05100}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2211.05100}.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{bpe}
R.~Sennrich, B.~Haddow, and A.~Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers}. The Association for Computer Linguistics, 2016.
\newblock \doi{10.18653/V1/P16-1162}.
\newblock URL \url{https://doi.org/10.18653/v1/p16-1162}.

\bibitem[Shazeer(2019)]{mqa}
N.~Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{CoRR}, abs/1911.02150, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.02150}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{moe}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~V. Le, G.~E. Hinton, and J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1ckMDqlg}.

\bibitem[Shen et~al.(2023)Shen, Hou, Zhou, Du, Longpre, Wei, Chung, Zoph, Fedus, Chen, Vu, Wu, Chen, Webson, Li, Zhao, Yu, Keutzer, Darrell, and Zhou]{flan_moe}
S.~Shen, L.~Hou, Y.~Zhou, N.~Du, S.~Longpre, J.~Wei, H.~W. Chung, B.~Zoph, W.~Fedus, X.~Chen, T.~Vu, Y.~Wu, W.~Chen, A.~Webson, Y.~Li, V.~Zhao, H.~Yu, K.~Keutzer, T.~Darrell, and D.~Zhou.
\newblock Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts.
\newblock \emph{CoRR}, abs/2305.14705, 2023.
\newblock \doi{10.48550/ARXIV.2305.14705}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.14705}.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{megatron}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{bbh}
M.~Suzgun, N.~Scales, N.~Sch{\"a}rli, S.~Gehrmann, Y.~Tay, H.~W. Chung, A.~Chowdhery, Q.~V. Le, E.~H. Chi, D.~Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Tillet et~al.(2019)Tillet, Kung, and Cox]{triton}
P.~Tillet, H.~T. Kung, and D.~Cox.
\newblock Triton: An intermediate language and compiler for tiled neural network computations.
\newblock In \emph{Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages}, MAPL 2019, page 10â€“19, New York, NY, USA, 2019. Association for Computing Machinery.
\newblock ISBN 9781450367196.
\newblock \doi{10.1145/3315508.3329973}.
\newblock URL \url{https://doi.org/10.1145/3315508.3329973}.

\bibitem[Together-AI(2023)]{redpajama}
Together-AI.
\newblock Redpajama-data: An open source recipe to reproduce llama training dataset, April 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.~Lachaux, T.~Lacroix, B.~Rozi{\`{e}}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin, E.~Grave, and G.~Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, abs/2302.13971, 2023{\natexlab{a}}.
\newblock \doi{10.48550/arXiv.2302.13971}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~Canton{-}Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.~Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2307.09288}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.09288}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017}, pages 5998--6008, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Wang and Komatsuzaki(2021)]{gptj}
B.~Wang and A.~Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Xu et~al.(2020)Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian, Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou, Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and Lan]{clue}
L.~Xu, H.~Hu, X.~Zhang, L.~Li, C.~Cao, Y.~Li, Y.~Xu, K.~Sun, D.~Yu, C.~Yu, Y.~Tian, Q.~Dong, W.~Liu, B.~Shi, Y.~Cui, J.~Li, J.~Zeng, R.~Wang, W.~Xie, Y.~Li, Y.~Patterson, Z.~Tian, Y.~Zhang, H.~Zhou, S.~Liu, Z.~Zhao, Q.~Zhao, C.~Yue, X.~Zhang, Z.~Yang, K.~Richardson, and Z.~Lan.
\newblock {CLUE:} {A} chinese language understanding evaluation benchmark.
\newblock In D.~Scott, N.~Bel, and C.~Zong, editors, \emph{Proceedings of the 28th International Conference on Computational Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13, 2020}, pages 4762--4772. International Committee on Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.COLING-MAIN.419}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.coling-main.419}.

\bibitem[Xue et~al.(2023)Xue, Zheng, Fu, Ni, Zheng, Zhou, and You]{openmoe}
F.~Xue, Z.~Zheng, Y.~Fu, J.~Ni, Z.~Zheng, W.~Zhou, and Y.~You.
\newblock Openmoe: Open mixture-of-experts language models.
\newblock \url{https://github.com/XueFuzhao/OpenMoE}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In A.~Korhonen, D.~R. Traum, and L.~M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 4791--4800. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/p19-1472}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1472}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster, D.~Simig, P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.

\bibitem[Zheng et~al.(2019)Zheng, Huang, and Sun]{chid}
C.~Zheng, M.~Huang, and A.~Sun.
\newblock Chid: {A} large-scale chinese idiom dataset for cloze test.
\newblock In A.~Korhonen, D.~R. Traum, and L.~M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 778--787. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/P19-1075}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1075}.

\bibitem[Zhou et~al.(2022)Zhou, Lei, Liu, Du, Huang, Zhao, Dai, Chen, Le, and Laudon]{ec}
Y.~Zhou, T.~Lei, H.~Liu, N.~Du, Y.~Huang, V.~Zhao, A.~M. Dai, Z.~Chen, Q.~V. Le, and J.~Laudon.
\newblock Mixture-of-experts with expert choice routing.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html}.

\bibitem[Zoph(2022)]{st_moe}
B.~Zoph.
\newblock Designing effective sparse expert models.
\newblock In \emph{{IEEE} International Parallel and Distributed Processing Symposium, {IPDPS} Workshops 2022, Lyon, France, May 30 - June 3, 2022}, page 1044. {IEEE}, 2022.
\newblock URL \url{https://doi.org/10.1109/IPDPSW55747.2022.00171}.

\end{thebibliography}
