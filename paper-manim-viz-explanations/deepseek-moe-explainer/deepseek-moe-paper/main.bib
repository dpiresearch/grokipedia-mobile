@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Volume 1 (Long and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
}

@article{gpt,
  author    = {Alec Radford and
              Karthik Narasimhan and
              Tim Salimans and
              Ilya Sutskever},
  title     = {Improving language understanding by generative pre-training},
  url       = {http://openai-assets.s3.amazonaws.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  year      = {2018}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  year={2019},
  url={http://www.persagen.com/files/misc/radford2019language.pdf}
}

@inproceedings{gpt3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{transformer,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017},
  pages     = {5998--6008},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{electra,
  author    = {Kevin Clark and
               Minh{-}Thang Luong and
               Quoc V. Le and
               Christopher D. Manning},
  title     = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than Generators},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=r1xMH1BtvB}
}

@inproceedings{unilmv2,
  author    = {Hangbo Bao and
               Li Dong and
               Furu Wei and
               Wenhui Wang and
               Nan Yang and
               Xiaodong Liu and
               Yu Wang and
               Jianfeng Gao and
               Songhao Piao and
               Ming Zhou and
               Hsiao{-}Wuen Hon},
  title     = {UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {642--652},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/bao20a.html}
}

@inproceedings{unilm,
  author    = {Li Dong and
               Nan Yang and
               Wenhui Wang and
               Furu Wei and
               Xiaodong Liu and
               Yu Wang and
               Jianfeng Gao and
               Ming Zhou and
               Hsiao{-}Wuen Hon},
  title     = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019},
  pages     = {13042--13054},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html}
}

@inproceedings{moe,
  author    = {Noam Shazeer and
               Azalia Mirhoseini and
               Krzysztof Maziarz and
               Andy Davis and
               Quoc V. Le and
               Geoffrey E. Hinton and
               Jeff Dean},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1ckMDqlg},
}

@article{switch,
  author    = {William Fedus and
               Barret Zoph and
               Noam Shazeer},
  title     = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal   = {CoRR},
  volume    = {abs/2101.03961},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.03961},
  archivePrefix = {arXiv},
  eprint    = {2101.03961},
}

@inproceedings{base,
  author    = {Mike Lewis and
               Shruti Bhosale and
               Tim Dettmers and
               Naman Goyal and
               Luke Zettlemoyer},
  title     = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {6265--6274},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/lewis21a.html},
}

@article{hash,
  author    = {Stephen Roller and
               Sainbayar Sukhbaatar and
               Arthur Szlam and
               Jason Weston},
  title     = {Hash Layers For Large Sparse Models},
  journal   = {CoRR},
  volume    = {abs/2106.04426},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.04426},
  archivePrefix = {arXiv},
  eprint    = {2106.04426},
}

@inproceedings{gshard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=qrwe7XHTmYb},
}

@article{auction,
  author    = {Dimitri P. Bertsekas},
  title     = {Auction algorithms for network flow problems: {A} tutorial introduction},
  journal   = {Computational Optimization and Applications},
  volume    = {1},
  number    = {1},
  pages     = {7--66},
  year      = {1992},
  url       = {https://doi.org/10.1007/BF00247653},
  doi       = {10.1007/BF00247653},
}

@inproceedings{cc100,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, {ACL} 2020},
  pages     = {8440--8451},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
}

@inproceedings{zcode,
  author    = {Yiren Wang and
               ChengXiang Zhai and
               Hany Hassan},
  title     = {Multi-task Learning for Multilingual Neural Machine Translation},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020},
  pages     = {1022--1034},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-main.75/},
}

@article{xlmt,
  author    = {Shuming Ma and
               Jian Yang and
               Haoyang Huang and
               Zewen Chi and
               Li Dong and
               Dongdong Zhang and
               Hany Hassan Awadalla and
               Alexandre Muzio and
               Akiko Eriguchi and
               Saksham Singhal and
               Xia Song and
               Arul Menezes and
               Furu Wei},
  title     = {{XLM-T:} Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders},
  journal   = {CoRR},
  volume    = {abs/2012.15547},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.15547},
}

@article{ori_moe1,
  author    = {Robert A. Jacobs and
               Michael I. Jordan and
               Steven J. Nowlan and
               Geoffrey E. Hinton},
  title     = {Adaptive Mixtures of Local Experts},
  journal   = {Neural Computing},
  volume    = {3},
  number    = {1},
  pages     = {79--87},
  year      = {1991},
  url       = {https://doi.org/10.1162/neco.1991.3.1.79},
}

@article{ori_moe2,
  author    = {Michael I. Jordan and
               Robert A. Jacobs},
  title     = {Hierarchical Mixtures of Experts and the {EM} Algorithm},
  journal   = {Neural Computing},
  volume    = {6},
  number    = {2},
  pages     = {181--214},
  year      = {1994},
  url       = {https://doi.org/10.1162/neco.1994.6.2.181},
}

@article{lstm,
  author    = {Sepp Hochreiter and
               J{\"{u}}rgen Schmidhuber},
  title     = {Long Short-Term Memory},
  journal   = {Neural Computing},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  url       = {https://doi.org/10.1162/neco.1997.9.8.1735},
}

@inproceedings{sentence_piece,
  author    = {Taku Kudo and
               John Richardson},
  title     = {SentencePiece: {A} simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural  Language Processing, {EMNLP} 2018},
  pages     = {66--71},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/d18-2012},
}

@article{llama,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.13971},
  doi          = {10.48550/arXiv.2302.13971},
}

@article{sparks_of_agi,
  author       = {S{\'{e}}bastien Bubeck and
                  Varun Chandrasekaran and
                  Ronen Eldan and
                  Johannes Gehrke and
                  Eric Horvitz and
                  Ece Kamar and
                  Peter Lee and
                  Yin Tat Lee and
                  Yuanzhi Li and
                  Scott M. Lundberg and
                  Harsha Nori and
                  Hamid Palangi and
                  Marco T{\'{u}}lio Ribeiro and
                  Yi Zhang},
  title        = {Sparks of Artificial General Intelligence: Early experiments with
                  {GPT-4}},
  journal      = {CoRR},
  volume       = {abs/2303.12712},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.12712},
  doi          = {10.48550/arXiv.2303.12712},
}

@article{gpt4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/arXiv.2303.08774},
}

@article{scaling_law,
  author       = {Jordan Hoffmann and
                  Sebastian Borgeaud and
                  Arthur Mensch and
                  Elena Buchatskaya and
                  Trevor Cai and
                  Eliza Rutherford and
                  Diego de Las Casas and
                  Lisa Anne Hendricks and
                  Johannes Welbl and
                  Aidan Clark and
                  Tom Hennigan and
                  Eric Noland and
                  Katie Millican and
                  George van den Driessche and
                  Bogdan Damoc and
                  Aurelia Guy and
                  Simon Osindero and
                  Karen Simonyan and
                  Erich Elsen and
                  Jack W. Rae and
                  Oriol Vinyals and
                  Laurent Sifre},
  title        = {Training Compute-Optimal Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2203.15556},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.15556},
  doi          = {10.48550/arXiv.2203.15556},
}

@inproceedings{glam,
  author       = {Nan Du and
                  Yanping Huang and
                  Andrew M. Dai and
                  Simon Tong and
                  Dmitry Lepikhin and
                  Yuanzhong Xu and
                  Maxim Krikun and
                  Yanqi Zhou and
                  Adams Wei Yu and
                  Orhan Firat and
                  Barret Zoph and
                  Liam Fedus and
                  Maarten P. Bosma and
                  Zongwei Zhou and
                  Tao Wang and
                  Yu Emma Wang and
                  Kellie Webster and
                  Marie Pellat and
                  Kevin Robinson and
                  Kathleen S. Meier{-}Hellstern and
                  Toju Duke and
                  Lucas Dixon and
                  Kun Zhang and
                  Quoc V. Le and
                  Yonghui Wu and
                  Zhifeng Chen and
                  Claire Cui},
  title        = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {5547--5569},
  publisher    = {{PMLR}},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v162/du22c.html},
}

@inproceedings{st_moe,
  author       = {Barret Zoph},
  title        = {Designing Effective Sparse Expert Models},
  booktitle    = {{IEEE} International Parallel and Distributed Processing Symposium,
                  {IPDPS} Workshops 2022, Lyon, France, May 30 - June 3, 2022},
  pages        = {1044},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/IPDPSW55747.2022.00171},
}

@misc{haillm,
    title = {HAI-LLM: An Efficient and Lightweight Tool for Training Large Models} ,
    author = {High-Flyer},
    institution = {High-Flyer},
    url = {https://www.high-flyer.cn/en/blog/hai-llm},
    year = {2023}
}

@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{bbh,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{clue,
  author       = {Liang Xu and
                  Hai Hu and
                  Xuanwei Zhang and
                  Lu Li and
                  Chenjie Cao and
                  Yudong Li and
                  Yechen Xu and
                  Kai Sun and
                  Dian Yu and
                  Cong Yu and
                  Yin Tian and
                  Qianqian Dong and
                  Weitang Liu and
                  Bo Shi and
                  Yiming Cui and
                  Junyi Li and
                  Jun Zeng and
                  Rongzhao Wang and
                  Weijian Xie and
                  Yanting Li and
                  Yina Patterson and
                  Zuoyu Tian and
                  Yiwen Zhang and
                  He Zhou and
                  Shaoweihua Liu and
                  Zhe Zhao and
                  Qipeng Zhao and
                  Cong Yue and
                  Xinrui Zhang and
                  Zhengliang Yang and
                  Kyle Richardson and
                  Zhenzhong Lan},
  editor       = {Donia Scott and
                  N{\'{u}}ria Bel and
                  Chengqing Zong},
  title        = {{CLUE:} {A} Chinese Language Understanding Evaluation Benchmark},
  booktitle    = {Proceedings of the 28th International Conference on Computational
                  Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13,
                  2020},
  pages        = {4762--4772},
  publisher    = {International Committee on Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.coling-main.419},
  doi          = {10.18653/V1/2020.COLING-MAIN.419},
}

@article{cmmlu,
  title={{CMMLU}: Measuring massive multitask language understanding in {Chinese}},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}

@inproceedings{chid,
  author       = {Chujie Zheng and
                  Minlie Huang and
                  Aixin Sun},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {ChID: {A} Large-scale Chinese IDiom Dataset for Cloze Test},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {778--787},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1075},
  doi          = {10.18653/V1/P19-1075},
}

@inproceedings{race,
  author       = {Guokun Lai and
                  Qizhe Xie and
                  Hanxiao Liu and
                  Yiming Yang and
                  Eduard H. Hovy},
  editor       = {Martha Palmer and
                  Rebecca Hwa and
                  Sebastian Riedel},
  title        = {{RACE:} Large-scale ReAding Comprehension Dataset From Examinations},
  booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
                  9-11, 2017},
  pages        = {785--794},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/d17-1082},
  doi          = {10.18653/V1/D17-1082},
}

@inproceedings{drop,
  author       = {Dheeru Dua and
                  Yizhong Wang and
                  Pradeep Dasigi and
                  Gabriel Stanovsky and
                  Sameer Singh and
                  Matt Gardner},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{DROP:} {A} Reading Comprehension Benchmark Requiring Discrete Reasoning
                  Over Paragraphs},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2368--2378},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1246},
  doi          = {10.18653/V1/N19-1246},
}

@article{ceval,
  title={{C-Eval}: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and others},
  journal={arXiv preprint arXiv:2305.08322},
  year={2023}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{pile,
  title={The {Pile}: An {800GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{agieval,
  author       = {Wanjun Zhong and
                  Ruixiang Cui and
                  Yiduo Guo and
                  Yaobo Liang and
                  Shuai Lu and
                  Yanlin Wang and
                  Amin Saied and
                  Weizhu Chen and
                  Nan Duan},
  title        = {{AGIEval}: {A} Human-Centric Benchmark for Evaluating Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.06364},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.06364},
  doi          = {10.48550/arXiv.2304.06364},
}

@inproceedings{hellaswag,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1472},
  doi          = {10.18653/v1/p19-1472},
}

@inproceedings{piqa,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/aaai.v34i05.6239},
}

@article{siqa,
  author       = {Maarten Sap and
                  Hannah Rashkin and
                  Derek Chen and
                  Ronan Le Bras and
                  Yejin Choi},
  title        = {{SocialIQA}: Commonsense Reasoning about Social Interactions},
  journal      = {CoRR},
  volume       = {abs/1904.09728},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09728},
}

@article{gaokao-bench,
  author       = {Xiaotian Zhang and
                  Chunyang Li and
                  Yi Zong and
                  Zhengyu Ying and
                  Liang He and
                  Xipeng Qiu},
  title        = {Evaluating the Performance of Large Language Models on {GAOKAO} Benchmark},
  journal      = {CoRR},
  volume       = {abs/2305.12474},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.12474},
  doi          = {10.48550/arXiv.2305.12474},
}

@article{arc,
  author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
  title        = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning
                  Challenge},
  journal      = {CoRR},
  volume       = {abs/1803.05457},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.05457},
}

@inproceedings{boolq,
  author       = {Christopher Clark and
                  Kenton Lee and
                  Ming{-}Wei Chang and
                  Tom Kwiatkowski and
                  Michael Collins and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2924--2936},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1300},
  doi          = {10.18653/v1/n19-1300},
}

@article{codex,
  author       = {Mark Chen and
                  Jerry Tworek and
                  Heewoo Jun and
                  Qiming Yuan and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jared Kaplan and
                  Harrison Edwards and
                  Yuri Burda and
                  Nicholas Joseph and
                  Greg Brockman and
                  Alex Ray and
                  Raul Puri and
                  Gretchen Krueger and
                  Michael Petrov and
                  Heidy Khlaaf and
                  Girish Sastry and
                  Pamela Mishkin and
                  Brooke Chan and
                  Scott Gray and
                  Nick Ryder and
                  Mikhail Pavlov and
                  Alethea Power and
                  Lukasz Kaiser and
                  Mohammad Bavarian and
                  Clemens Winter and
                  Philippe Tillet and
                  Felipe Petroski Such and
                  Dave Cummings and
                  Matthias Plappert and
                  Fotios Chantzis and
                  Elizabeth Barnes and
                  Ariel Herbert{-}Voss and
                  William Hebgen Guss and
                  Alex Nichol and
                  Alex Paino and
                  Nikolas Tezak and
                  Jie Tang and
                  Igor Babuschkin and
                  Suchir Balaji and
                  Shantanu Jain and
                  William Saunders and
                  Christopher Hesse and
                  Andrew N. Carr and
                  Jan Leike and
                  Joshua Achiam and
                  Vedant Misra and
                  Evan Morikawa and
                  Alec Radford and
                  Matthew Knight and
                  Miles Brundage and
                  Mira Murati and
                  Katie Mayer and
                  Peter Welinder and
                  Bob McGrew and
                  Dario Amodei and
                  Sam McCandlish and
                  Ilya Sutskever and
                  Wojciech Zaremba},
  title        = {Evaluating Large Language Models Trained on Code},
  journal      = {CoRR},
  volume       = {abs/2107.03374},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03374},
  eprinttype    = {arXiv},
  eprint       = {2107.03374},
}

@article{nq,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}

@article{triviaqa,
       author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},
                 Daniel and {Zettlemoyer}, Luke},
        title = "{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}",
      journal = {arXiv e-prints},
         year = 2017,
          eid = {arXiv:1705.03551},
        pages = {arXiv:1705.03551},
archivePrefix = {arXiv},
       eprint = {1705.03551},
}


@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{megatron2,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{megatron3,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{zero,
  author       = {Samyam Rajbhandari and
                  Jeff Rasley and
                  Olatunji Ruwase and
                  Yuxiong He},
  editor       = {Christine Cuicchi and
                  Irene Qualters and
                  William T. Kramer},
  title        = {ZeRO: memory optimizations toward training trillion parameter models},
  booktitle    = {Proceedings of the International Conference for High Performance Computing,
                  Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta,
                  Georgia, USA, November 9-19, 2020},
  pages        = {20},
  publisher    = {{IEEE/ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1109/SC41405.2020.00024},
  doi          = {10.1109/SC41405.2020.00024},
}

@article{pipedream,
  author       = {Aaron Harlap and
                  Deepak Narayanan and
                  Amar Phanishayee and
                  Vivek Seshadri and
                  Nikhil R. Devanur and
                  Gregory R. Ganger and
                  Phillip B. Gibbons},
  title        = {PipeDream: Fast and Efficient Pipeline Parallel {DNN} Training},
  journal      = {CoRR},
  volume       = {abs/1806.03377},
  year         = {2018},
  url          = {http://arxiv.org/abs/1806.03377},
}

@inproceedings{bpe,
  author       = {Rico Sennrich and
                  Barry Haddow and
                  Alexandra Birch},
  title        = {Neural Machine Translation of Rare Words with Subword Units},
  booktitle    = {Proceedings of the 54th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume
                  1: Long Papers},
  publisher    = {The Association for Computer Linguistics},
  year         = {2016},
  url          = {https://doi.org/10.18653/v1/p16-1162},
  doi          = {10.18653/V1/P16-1162},
}

@inproceedings{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7},
}

@misc{winogrande,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/arXiv.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288}
}

@inproceedings{stablemoe,
  author       = {Damai Dai and
                  Li Dong and
                  Shuming Ma and
                  Bo Zheng and
                  Zhifang Sui and
                  Baobao Chang and
                  Furu Wei},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {StableMoE: Stable Routing Strategy for Mixture of Experts},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {7085--7095},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.489},
  doi          = {10.18653/V1/2022.ACL-LONG.489},
}

@inproceedings{ec,
  author       = {Yanqi Zhou and
                  Tao Lei and
                  Hanxiao Liu and
                  Nan Du and
                  Yanping Huang and
                  Vincent Zhao and
                  Andrew M. Dai and
                  Zhifeng Chen and
                  Quoc V. Le and
                  James Laudon},
  title        = {Mixture-of-Experts with Expert Choice Routing},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html},
}

@inproceedings{base,
  author    = {Mike Lewis and
               Shruti Bhosale and
               Tim Dettmers and
               Naman Goyal and
               Luke Zettlemoyer},
  title     = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {6265--6274},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/lewis21a.html},
}

@article{m6,
  author       = {Junyang Lin and
                  Rui Men and
                  An Yang and
                  Chang Zhou and
                  Ming Ding and
                  Yichang Zhang and
                  Peng Wang and
                  Ang Wang and
                  Le Jiang and
                  Xianyan Jia and
                  Jie Zhang and
                  Jianwei Zhang and
                  Xu Zou and
                  Zhikang Li and
                  Xiaodong Deng and
                  Jie Liu and
                  Jinbao Xue and
                  Huiling Zhou and
                  Jianxin Ma and
                  Jin Yu and
                  Yong Li and
                  Wei Lin and
                  Jingren Zhou and
                  Jie Tang and
                  Hongxia Yang},
  title        = {{M6:} {A} Chinese Multimodal Pretrainer},
  journal      = {CoRR},
  volume       = {abs/2103.00823},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.00823},
}

@article{pangu_sigma,
  author       = {Xiaozhe Ren and
                  Pingyi Zhou and
                  Xinfan Meng and
                  Xinjing Huang and
                  Yadao Wang and
                  Weichao Wang and
                  Pengfei Li and
                  Xiaoda Zhang and
                  Alexander Podolskiy and
                  Grigory Arshinov and
                  Andrey Bout and
                  Irina Piontkovskaya and
                  Jiansheng Wei and
                  Xin Jiang and
                  Teng Su and
                  Qun Liu and
                  Jun Yao},
  title        = {PanGu-{\(\Sigma\)}: Towards Trillion Parameter Language Model with
                  Sparse Heterogeneous Computing},
  journal      = {CoRR},
  volume       = {abs/2303.10845},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.10845},
}

@inproceedings{truthfulqa,
  author       = {Stephanie Lin and
                  Jacob Hilton and
                  Owain Evans},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {3214--3252},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.229},
  doi          = {10.18653/V1/2022.ACL-LONG.229},
}

@article{deepseek_llm,
  author = {DeepSeek-AI},
  title = {DeepSeek LLM: Scaling Open-Source Language Models with Longtermism},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@inproceedings{kn,
  author       = {Damai Dai and
                  Li Dong and
                  Yaru Hao and
                  Zhifang Sui and
                  Baobao Chang and
                  Furu Wei},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {Knowledge Neurons in Pretrained Transformers},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {8493--8502},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.581},
  doi          = {10.18653/V1/2022.ACL-LONG.581},
  timestamp    = {Mon, 01 Aug 2022 16:27:52 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/DaiDHSCW22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mqa,
  author       = {Noam Shazeer},
  title        = {Fast Transformer Decoding: One Write-Head is All You Need},
  journal      = {CoRR},
  volume       = {abs/1911.02150},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.02150},
}

@article{qwen,
  author       = {Jinze Bai and
                  Shuai Bai and
                  Yunfei Chu and
                  Zeyu Cui and
                  Kai Dang and
                  Xiaodong Deng and
                  Yang Fan and
                  Wenbin Ge and
                  Yu Han and
                  Fei Huang and
                  Binyuan Hui and
                  Luo Ji and
                  Mei Li and
                  Junyang Lin and
                  Runji Lin and
                  Dayiheng Liu and
                  Gao Liu and
                  Chengqiang Lu and
                  Keming Lu and
                  Jianxin Ma and
                  Rui Men and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Chuanqi Tan and
                  Sinan Tan and
                  Jianhong Tu and
                  Peng Wang and
                  Shijie Wang and
                  Wei Wang and
                  Shengguang Wu and
                  Benfeng Xu and
                  Jin Xu and
                  An Yang and
                  Hao Yang and
                  Jian Yang and
                  Shusheng Yang and
                  Yang Yao and
                  Bowen Yu and
                  Hongyi Yuan and
                  Zheng Yuan and
                  Jianwei Zhang and
                  Xingxuan Zhang and
                  Yichang Zhang and
                  Zhenru Zhang and
                  Chang Zhou and
                  Jingren Zhou and
                  Xiaohuan Zhou and
                  Tianhang Zhu},
  title        = {Qwen Technical Report},
  journal      = {CoRR},
  volume       = {abs/2309.16609},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.16609},
  doi          = {10.48550/ARXIV.2309.16609},
}

@article{bloom,
  author       = {Teven Le Scao and
                  Angela Fan and
                  Christopher Akiki and
                  Ellie Pavlick and
                  Suzana Ilic and
                  Daniel Hesslow and
                  Roman Castagn{\'{e}} and
                  Alexandra Sasha Luccioni and
                  Fran{\c{c}}ois Yvon and
                  Matthias Gall{\'{e}} and
                  Jonathan Tow and
                  Alexander M. Rush and
                  Stella Biderman and
                  Albert Webson and
                  Pawan Sasanka Ammanamanchi and
                  Thomas Wang and
                  Beno{\^{\i}}t Sagot and
                  Niklas Muennighoff and
                  Albert Villanova del Moral and
                  Olatunji Ruwase and
                  Rachel Bawden and
                  Stas Bekman and
                  Angelina McMillan{-}Major and
                  Iz Beltagy and
                  Huu Nguyen and
                  Lucile Saulnier and
                  Samson Tan and
                  Pedro Ortiz Suarez and
                  Victor Sanh and
                  Hugo Lauren{\c{c}}on and
                  Yacine Jernite and
                  Julien Launay and
                  Margaret Mitchell and
                  Colin Raffel and
                  Aaron Gokaslan and
                  Adi Simhi and
                  Aitor Soroa and
                  Alham Fikri Aji and
                  Amit Alfassy and
                  Anna Rogers and
                  Ariel Kreisberg Nitzav and
                  Canwen Xu and
                  Chenghao Mou and
                  Chris Emezue and
                  Christopher Klamm and
                  Colin Leong and
                  Daniel van Strien and
                  David Ifeoluwa Adelani and
                  et al.},
  title        = {{BLOOM:} {A} 176B-Parameter Open-Access Multilingual Language Model},
  journal      = {CoRR},
  volume       = {abs/2211.05100},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05100},
  doi          = {10.48550/ARXIV.2211.05100},
}

@misc{gpt_neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this misc, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@misc{gptj,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@inproceedings{pythia,
  author       = {Stella Biderman and
                  Hailey Schoelkopf and
                  Quentin Gregory Anthony and
                  Herbie Bradley and
                  Kyle O'Brien and
                  Eric Hallahan and
                  Mohammad Aflah Khan and
                  Shivanshu Purohit and
                  USVSN Sai Prashanth and
                  Edward Raff and
                  Aviya Skowron and
                  Lintang Sutawika and
                  Oskar van der Wal},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Pythia: {A} Suite for Analyzing Large Language Models Across Training
                  and Scaling},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {2397--2430},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/biderman23a.html},
}

@misc{falcon,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@misc{openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = "May",
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@misc{redpajama,
  author = {Together-AI},
  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = "April",
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@misc{opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yi,
  author = {01-AI},
  title = {Building the Next Generation of Open-Source and Bilingual LLMs},
  url = {https://github.com/01-ai/Yi},
  year = 2023,
}

@inproceedings{moe_ft_bad,
  author       = {Mikel Artetxe and
                  Shruti Bhosale and
                  Naman Goyal and
                  Todor Mihaylov and
                  Myle Ott and
                  Sam Shleifer and
                  Xi Victoria Lin and
                  Jingfei Du and
                  Srinivasan Iyer and
                  Ramakanth Pasunuru and
                  Giridharan Anantharaman and
                  Xian Li and
                  Shuohui Chen and
                  Halil Akin and
                  Mandeep Baines and
                  Louis Martin and
                  Xing Zhou and
                  Punit Singh Koura and
                  Brian O'Horo and
                  Jeffrey Wang and
                  Luke Zettlemoyer and
                  Mona T. Diab and
                  Zornitsa Kozareva and
                  Veselin Stoyanov},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {Efficient Large Scale Language Modeling with Mixtures of Experts},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
                  December 7-11, 2022},
  pages        = {11699--11732},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.emnlp-main.804},
  doi          = {10.18653/V1/2022.EMNLP-MAIN.804},
}

@article{flan_moe,
  author       = {Sheng Shen and
                  Le Hou and
                  Yanqi Zhou and
                  Nan Du and
                  Shayne Longpre and
                  Jason Wei and
                  Hyung Won Chung and
                  Barret Zoph and
                  William Fedus and
                  Xinyun Chen and
                  Tu Vu and
                  Yuexin Wu and
                  Wuyang Chen and
                  Albert Webson and
                  Yunxuan Li and
                  Vincent Zhao and
                  Hongkun Yu and
                  Kurt Keutzer and
                  Trevor Darrell and
                  Denny Zhou},
  title        = {Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse
                  Mixture of Experts},
  journal      = {CoRR},
  volume       = {abs/2305.14705},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.14705},
  doi          = {10.48550/ARXIV.2305.14705},
}

@misc{math,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{triton,
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
title = {Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
year = {2019},
isbn = {9781450367196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315508.3329973},
doi = {10.1145/3315508.3329973},
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts  usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {1019},
numpages = {10},
keywords = {GPU, compiler, neural networks},
location = {Phoenix, AZ, USA},
series = {MAPL 2019}
}

@inproceedings{deepspeed_moe,
  author       = {Samyam Rajbhandari and
                  Conglong Li and
                  Zhewei Yao and
                  Minjia Zhang and
                  Reza Yazdani Aminabadi and
                  Ammar Ahmad Awan and
                  Jeff Rasley and
                  Yuxiong He},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training
                  to Power Next-Generation {AI} Scale},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {18332--18346},
  publisher    = {{PMLR}},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v162/rajbhandari22a.html},
}

@article{under_specialization_1,
  author       = {Yamuna Krishnamurthy and
                  Chris Watkins and
                  Thomas G{\"{a}}rtner},
  title        = {Improving Expert Specialization in Mixture of Experts},
  journal      = {CoRR},
  volume       = {abs/2302.14703},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.14703},
  doi          = {10.48550/ARXIV.2302.14703},
}

@inproceedings{under_specialization_2,
  author       = {Sarthak Mittal and
                  Yoshua Bengio and
                  Guillaume Lajoie},
  title        = {Is a Modular Architecture Enough?},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/b8d1d741f137d9b6ac4f3c1683791e4a-Abstract-Conference.html},
}

@misc{openmoe,
  author = {Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You},
  title = {OpenMoE: Open Mixture-of-Experts Language Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/XueFuzhao/OpenMoE}},
}